---
title: MPC与强化学习
date: 2024-05-31 12:03:05
tags: 强化学习
---

## 1.动机

由于实验室中的某些做传统算法的使用了mpc作为他们的策略生成方式，网上有人把mpc算法也划分到了model-based RL中，感觉比较好奇，就具体研究一下他们到底有什么不同。

## 2.模型预测控制算法（mpc）

首先采用GPT生成MPC的数学推导。它将MPC算法分成了6个过程

### 2.1GPT生成的理论推导

#### a 系统建模

考虑一个离散时间线性系统，其状态方程为：
$$
x(k+1)=A x(k)+B u(k)
$$
其中：

- $x(k)$是时刻$k$的系统状态向量。

- $u(k)$是时刻$k$的控制输入向量。

- $A,B$是系统矩阵，描述系统的动力学特性。

#### b 预测模型

根据系统模型，预测时域上N步内的未来状态。其预测状态输出的递推公式为：
$$
x(k+i+1 \mid k)=A x(k+i \mid k)+B u(k+i \mid k), \quad i=0,1, \ldots, N-1
$$

#### c 优化目标函数

MPC的目标是通过优化控制输入$u$,使得最小化目标函数。常用的目标函数形式为：

#### d 约束条件

控制问题还包含一些约束条件，比如输入和状态的物理约束：
$$
\begin{array}{ll}
x_{\min } \leq x(k+i \mid k) \leq x_{\max }, & i=0,1, \ldots, N \\
u_{\min } \leq u(k+i \mid k) \leq u_{\max }, & i=0,1, \ldots, N-1
\end{array}
$$

#### e 优化问题求解

将c,d组合起来，通常是一个二次规划问题(QP)，在线性系统下具有凸性质，可以高效求解。

#### f 执行控制输入

通过求解上述优化问题，得到控制输入序列$\{u(k \mid k), u(k+1 \mid k), \ldots, u(k+N-1 \mid k)\}$。实际执行时，仅应用当前时刻的控制输入$u(k \mid k)$，然后滚动到下一时刻，重复上述过程。

### 2.2总结

根据GPT生成的理论推导，可以得出其已经通过建模的方式得到了目标系统的状态转移方程，然后再根据任务要求设定目标函数和约束条件，求解出控制输入。

但是这种方法非常依赖于设计者的系统建模能力，也就是需要相当多的专家经验去拟合这个模型，且这个模型还得满足线性系统的条件，泛化能力太弱了。对于不同的场景就得去寻找不同的模型，学长做的从车辆规划底层的控制模型较为简单且已经很成熟了，经过了大量的时间验证；但是我所做的方向对于1架F16战斗机而言，其控制模型是及其复杂的非线性模型，如果要用的话只能用非线性模型预测控制算法，同时还存在其约束条件不明确，目标函数不明确的问题。

## 3.model-based RL

### 2.1理论推导

#### a 问题描述

对于强化学习肯定绕不开马尔可夫决策过程(MDP),因为RL的数学公式都是基于MDP来进行的推导的，如果脱离了就无法保证收敛性。

对于一个标准的马尔可夫决策过程来说有5个元素：

- **状态空间** $S$

- **动作空间** $A$

- **状态转移函数**$P\left(s^{\prime} \mid s, a\right)$：给定当前状态$s$和动作$a$，转移到下一状态$s^{\prime}$的概率。

- **奖励函数**$R(s, a)$：给定当前状态$s$和动作$a$后获得的即时奖励。

- **折扣因子**$\gamma \in[0,1)$：用于折扣未来奖励的影响。

#### b 环境模型建立

对于强化学习而言状态转移函数是未知的，但是对于环境而言必须是确定的。

但是这里有存在一个真实的模型$\hat{P}\left(s^{\prime} \mid s, a\right)$和$\hat{R}(s, a)$可供学习：
$$
\begin{aligned}
& \hat{P}\left(s^{\prime} \mid s, a\right) \approx P\left(s^{\prime} \mid s, a\right) \\
& \hat{R}(s, a) \approx R(s, a)
\end{aligned}
$$
那么通过这些模型收集经验数据$\left(s, a, s^{\prime}, r\right)$，然后通过监督学习的方式就可以进行估计。

#### c 策略优化

有了环境模型后，就很容易解决这个问题了。通过价值迭代方法、策略迭代方法或者蒙特卡洛树搜索方法都可以解决这个问题。



#### d 存在的问题

显然估计的环境模型与真实模型大概率存在误差，处理误差可以通过

- **不确定性估计**：使用贝叶斯方法或集成学习方法估计模型的不确定性，并在策略优化时考虑这种不确定性。

- **Dyna架构**：结合基于模型和无模型的方法，通过在真实环境中执行动作并使用学习到的模型进行模拟，来更新策略和模型。

- **模型重训练**：定期重新训练环境模型，以减小模型误差对策略优化的影响。









